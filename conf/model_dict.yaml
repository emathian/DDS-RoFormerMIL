#############################
### Config for base models###
#############################

clam_sb: 
  _target_: romil.models.model_clam.CLAM_SB
  n_classes: ${${task}.n_classes}
  input_dim: 1024
  hidden_dim: 512
  dropout: 0.25
  instance_classifiers: ${training_args.lightning_module.use_instance_loss}
  attention_net:
    _target_: romil.models.model_clam.Attn_Net_Gated
    hidden_dim: ${..hidden_dim}
    attention_dim: 256
    dropout: ${..dropout}
    n_classes: 1

#####################

clam_mb: 
  _target_: romil.models.model_clam.CLAM_MB
  n_classes: ${${task}.n_classes}
  input_dim: 1024
  hidden_dim: 512
  dropout: 0.25
  instance_classifiers: ${training_args.lightning_module.use_instance_loss}
  attention_net:
    _target_: romil.models.model_clam.Attn_Net_Gated
    hidden_dim: ${..hidden_dim}
    attention_dim: 256
    dropout: ${..dropout}
    n_classes: ${..n_classes}

#####################

RoPEAMIL_pixel:
  _target_: romil.models.RoMIL.RoPEAMIL
  input_dim: 1024
  hidden_dim: 512
  absolute_position_embeddings: False
  positional_encoder: 
    _target_: romil.models.position_embedding_modules.RoFormerEncoder
    _recursive_: False
    n_attention_block: 1
    n_embd: ${..hidden_dim}
    n_head: 8
    dropout: 0.25
    bias: True
    rope: True
    rope_freqs: pixel
    resid_dropout: 0.25
  mil_head: 
    _target_: romil.models.ABMIL_heads.ABMIL_MB
    n_classes: ${${task}.n_classes}
    input_dim: ${..hidden_dim}
    hidden_dim: ${..hidden_dim}
    dropout: 0.25
    instance_classifiers: False
    attention_net:
      _target_: romil.models.attention.ClassAttention
      attn_scores: False # Warning if true compute inneficient attention to store the weights
      hidden_dim: ${..hidden_dim}
      attention_dim: 256
      dropout: ${..dropout}
      n_classes: ${${task}.n_classes}
      n_head: 8
      bias: True

#####################

RoPEAMIL_no_rope:
  _target_: romil.models.RoMIL.RoPEAMIL
  input_dim: 1024
  hidden_dim: 512
  absolute_position_embeddings: False
  positional_encoder: 
    _target_: romil.models.position_embedding_modules.RoFormerEncoder
    n_attention_block: 1
    n_embd: ${..hidden_dim}
    n_head: 8
    dropout: 0.25
    bias: True
    rope: False
    rope_freqs: pixel
    resid_dropout: 0.25
  mil_head: 
    _target_: romil.models.ABMIL_heads.ABMIL_MB
    n_classes: ${${task}.n_classes}
    input_dim: ${..hidden_dim}
    hidden_dim: ${..hidden_dim}
    dropout: 0.25
    instance_classifiers: False
    attention_net:
      _target_: romil.models.attention.ClassAttention
      hidden_dim: ${..hidden_dim}
      attention_dim: 256
      dropout: ${..dropout}
      n_classes: ${${task}.n_classes}
      n_head: 8
      bias: True


#####################

RoPEDSMIL_pixel:
  _target_: romil.models.RoMIL.RoPEDSMIL
  input_dim: 1024
  hidden_dim: 512
  absolute_position_embeddings: False
  positional_encoder: 
    _target_: romil.models.position_embedding_modules.RoFormerEncoder
    n_attention_block: 1
    n_embd: ${..hidden_dim}
    n_head: 8
    dropout: 0.25
    bias: True
    rope: True
    rope_freqs: pixel
    resid_dropout: 0.25

  mil_head: 
    _target_: romil.models.dsmil.MILNet
    n_classes: ${${task}.n_classes}
    patch_classifier:
      _target_: romil.models.dsmil.FCLayer
      in_size: ${...hidden_dim}
      out_size:  ${${task}.n_classes}
    attention_net:
      _target_: romil.models.dsmil.BClassifier
      input_size: ${...hidden_dim}
      n_classes: ${${task}.n_classes}

#####################
# Config for BT projector d=128
BTRoPEAMIL_pixel:
  _target_: romil.models.RoMIL.RoPEAMIL
  input_dim: 128
  hidden_dim: 128
  absolute_position_embeddings: False
  positional_encoder: 
    _target_: romil.models.position_embedding_modules.RoFormerEncoder
    _recursive_: False
    n_attention_block: 1
    n_embd: ${..hidden_dim}
    n_head: 8
    dropout: 0
    bias: True
    rope: True
    rope_freqs: pixel
    resid_dropout: 0
  mil_head: 
    _target_: romil.models.ABMIL_heads.ABMIL_MB
    n_classes: ${${task}.n_classes}
    input_dim: ${..hidden_dim}
    hidden_dim: ${..hidden_dim}
    dropout: 0
    instance_classifiers: False
    attention_net:
      _target_: romil.models.attention.ClassAttention
      attn_scores: False # Warning if true compute inneficient attention to store the weights
      hidden_dim: ${..hidden_dim}
      attention_dim: 96
      dropout: ${..dropout}
      n_classes: ${${task}.n_classes}
      n_head: 8
      bias: True


# #####################
# # CaA1 CaA2 Overfitting
# BTRoPEAMIL_s_pixel:
#   _target_: romil.models.RoMIL.BTRoPEAMIL
#   input_dim: 128
#   hidden_dim: 128 # Not activate first linear layer
#   absolute_position_embeddings: False
#   positional_encoder: 
#     _target_: romil.models.position_embedding_modules.RoFormerEncoder
#     _recursive_: False
#     n_attention_block: 4
#     n_embd: ${..hidden_dim}
#     n_head: 16
#     dropout: 0.2
#     rope: True
#     rope_freqs: pixel
#     resid_dropout: 0.2
#   mil_head: 
#     _target_: romil.models.ABMIL_heads.ABMIL_MB
#     n_classes: ${${task}.n_classes}
#     input_dim: ${..hidden_dim}
#     hidden_dim: ${..hidden_dim}
#     dropout: 0.2
#     instance_classifiers: False
#     attention_net:
#       _target_: romil.models.attention.ClassAttention
#       attn_scores: True # Warning if true compute inneficient attention to store the weights
#       hidden_dim: ${..hidden_dim}
#       attention_dim: 64
#       dropout: ${..dropout}
#       n_classes: ${${task}.n_classes}
#       n_head: 8
#       bias: True

## CaA1 CaA2 Slight overfitting useful-auk-789 
# BTRoPEAMIL_s_pixel:
#   _target_: romil.models.RoMIL.BTRoPEAMIL
#   input_dim: 128
#   hidden_dim: 64 # Not activate first linear layer
#   absolute_position_embeddings: False
#   positional_encoder: 
#     _target_: romil.models.position_embedding_modules.RoFormerEncoder
#     _recursive_: False
#     n_attention_block: 4
#     n_embd: ${..hidden_dim}
#     n_head: 8
#     dropout: 0.2
#     rope: True
#     rope_freqs: pixel
#     resid_dropout: 0.2
#   mil_head: 
#     _target_: romil.models.ABMIL_heads.ABMIL_MB
#     n_classes: ${${task}.n_classes}
#     input_dim: ${..hidden_dim}
#     hidden_dim: ${..hidden_dim}
#     dropout: 0.2
#     instance_classifiers: False
#     attention_net:
#       _target_: romil.models.attention.ClassAttention
#       attn_scores: True # Warning if true compute inneficient attention to store the weights
#       hidden_dim: ${..hidden_dim}
#       attention_dim: 32
#       dropout: ${..dropout}
#       n_classes: ${${task}.n_classes}
#       n_head: 8
#       bias: True

## More slight overfit gaudy-crow-950 
# BTRoPEAMIL_s_pixel:
#   _target_: romil.models.RoMIL.BTRoPEAMIL
#   input_dim: 128
#   hidden_dim: 64 # Not activate first linear layer
#   absolute_position_embeddings: False
#   positional_encoder: 
#     _target_: romil.models.position_embedding_modules.RoFormerEncoder
#     _recursive_: False
#     n_attention_block: 2
#     n_embd: ${..hidden_dim}
#     n_head: 8
#     dropout: 0.3
#     rope: True
#     rope_freqs: pixel
#     resid_dropout: 0.2
#   mil_head: 
#     _target_: romil.models.ABMIL_heads.ABMIL_MB
#     n_classes: ${${task}.n_classes}
#     input_dim: ${..hidden_dim}
#     hidden_dim: ${..hidden_dim}
#     dropout: 0.3
#     instance_classifiers: False
#     attention_net:
#       _target_: romil.models.attention.ClassAttention
#       attn_scores: True # Warning if true compute inneficient attention to store the weights
#       hidden_dim: ${..hidden_dim}
#       attention_dim: 32
#       dropout: ${..dropout}
#       n_classes: ${${task}.n_classes}
#       n_head: 8
#       bias: True#

## enthused-mule-248 
# BTRoPEAMIL_s_pixel:
#   _target_: romil.models.RoMIL.BTRoPEAMIL
#   input_dim: 128
#   hidden_dim: 64 # Not activate first linear layer
#   absolute_position_embeddings: False
#   positional_encoder: 
#     _target_: romil.models.position_embedding_modules.RoFormerEncoder
#     _recursive_: False
#     n_attention_block: 2
#     n_embd: ${..hidden_dim}
#     n_head: 8
#     dropout: 0.3
#     rope: True
#     rope_freqs: pixel
#     resid_dropout: 0.2
#   mil_head: 
#     _target_: romil.models.ABMIL_heads.ABMIL_MB
#     n_classes: ${${task}.n_classes}
#     input_dim: ${..hidden_dim}
#     hidden_dim: ${..hidden_dim}
#     dropout: 0.3
#     instance_classifiers: False
#     attention_net:
#       _target_: romil.models.attention.ClassAttention
#       attn_scores: True # Warning if true compute inneficient attention to store the weights
#       hidden_dim: ${..hidden_dim}
#       attention_dim: 32
#       dropout: ${..dropout}
#       n_classes: ${${task}.n_classes}
#       n_head: 4
#       bias: True


# BTRoPEAMIL_s_pixel: #omniscient-fish-220 
#   _target_: romil.models.RoMIL.BTRoPEAMIL
#   input_dim: 128
#   hidden_dim: 64 # Not activate first linear layer
#   absolute_position_embeddings: False
#   positional_encoder: 
#     _target_: romil.models.position_embedding_modules.RoFormerEncoder
#     _recursive_: False
#     n_attention_block: 2
#     n_embd: ${..hidden_dim}
#     n_head: 4
#     dropout: 0.3
#     rope: True
#     rope_freqs: pixel
#     resid_dropout: 0.3
#   mil_head: 
#     _target_: romil.models.ABMIL_heads.ABMIL_MB
#     n_classes: ${${task}.n_classes}
#     input_dim: ${..hidden_dim}
#     hidden_dim: ${..hidden_dim}
#     dropout: 0.3
#     instance_classifiers: False
#     attention_net:
#       _target_: romil.models.attention.ClassAttention
#       attn_scores: True # Warning if true compute inneficient attention to store the weights
#       hidden_dim: ${..hidden_dim}
#       attention_dim: 32
#       dropout: ${..dropout}
#       n_classes: ${${task}.n_classes}
#       n_head: 4
#       bias: True



# BTRoPEAMIL_s_pixel: #wise-zebra-84 
#   _target_: romil.models.RoMIL.BTRoPEAMIL
#   input_dim: 128
#   hidden_dim: 64 # Not activate first linear layer
#   absolute_position_embeddings: False
#   positional_encoder: 
#     _target_: romil.models.position_embedding_modules.RoFormerEncoder
#     _recursive_: False
#     n_attention_block: 8
#     n_embd: ${..hidden_dim}
#     n_head: 8
#     dropout: 0.4
#     rope: True
#     rope_freqs: pixel
#     resid_dropout: 0.4
#   mil_head: 
#     _target_: romil.models.ABMIL_heads.ABMIL_MB
#     n_classes: ${${task}.n_classes}
#     input_dim: ${..hidden_dim}
#     hidden_dim: ${..hidden_dim}
#     dropout: 0.4
#     instance_classifiers: False
#     attention_net:
#       _target_: romil.models.attention.ClassAttention
#       attn_scores: True # Warning if true compute inneficient attention to store the weights
#       hidden_dim: ${..hidden_dim}
#       attention_dim: 48
#       dropout: ${..dropout}
#       n_classes: ${${task}.n_classes}
#       n_head: 8
#       bias: True



# BTRoPEAMIL_s_pixel: #calm-moose-545 
#   _target_: romil.models.RoMIL.BTRoPEAMIL
#   input_dim: 128
#   hidden_dim: 64 # Not activate first linear layer
#   absolute_position_embeddings: False
#   positional_encoder: 
#     _target_: romil.models.position_embedding_modules.RoFormerEncoder
#     _recursive_: False
#     n_attention_block: 2
#     n_embd: ${..hidden_dim}
#     n_head: 4
#     dropout: 0.4
#     rope: True
#     rope_freqs: pixel
#     resid_dropout: 0.4
#   mil_head: 
#     _target_: romil.models.ABMIL_heads.ABMIL_MB
#     n_classes: ${${task}.n_classes}
#     input_dim: ${..hidden_dim}
#     hidden_dim: ${..hidden_dim}
#     dropout: 0.4
#     instance_classifiers: False
#     attention_net:
#       _target_: romil.models.attention.ClassAttention
#       attn_scores: True # Warning if true compute inneficient attention to store the weights
#       hidden_dim: ${..hidden_dim}
#       attention_dim: 48
#       dropout: ${..dropout}
#       n_classes: ${${task}.n_classes}
#       n_head: 4
#       bias: True



# BTRoPEAMIL_s_pixel: #Selective shark | omniscient-fish-220  like bigger batch size = 32 
#   _target_: romil.models.RoMIL.BTRoPEAMIL
#   input_dim: 128
#   hidden_dim: 64 # Not activate first linear layer
#   absolute_position_embeddings: False
#   positional_encoder: 
#     _target_: romil.models.position_embedding_modules.RoFormerEncoder
#     _recursive_: False
#     n_attention_block: 2
#     n_embd: ${..hidden_dim}
#     n_head: 4
#     dropout: 0.3
#     rope: True
#     rope_freqs: pixel
#     resid_dropout: 0.35
#   mil_head: 
#     _target_: romil.models.ABMIL_heads.ABMIL_MB
#     n_classes: ${${task}.n_classes}
#     input_dim: ${..hidden_dim}
#     hidden_dim: ${..hidden_dim}
#     dropout: 0.3
#     instance_classifiers: False
#     attention_net:
#       _target_: romil.models.attention.ClassAttention
#       attn_scores: True # Warning if true compute inneficient attention to store the weights
#       hidden_dim: ${..hidden_dim}
#       attention_dim: 32
#       dropout: ${..dropout}
#       n_classes: ${${task}.n_classes}
#       n_head: 4
#       bias: True


BTRoPEAMIL_s_pixel: # redolent-goat-469  => Best model  ## & unbeat steed + Stylish mule
  _target_: romil.models.RoMIL.BTRoPEAMIL
  input_dim: 128
  hidden_dim: 64 # Not activate first linear layer
  absolute_position_embeddings: False
  positional_encoder: 
    _target_: romil.models.position_embedding_modules.RoFormerEncoder
    _recursive_: False
    n_attention_block: 6
    n_embd: ${..hidden_dim}
    n_head: 8
    dropout: 0.3
    rope: True
    rope_freqs: pixel
    resid_dropout: 0.3
  mil_head: 
    _target_: romil.models.ABMIL_heads.ABMIL_MB
    n_classes: ${${task}.n_classes}
    input_dim: ${..hidden_dim}
    hidden_dim: ${..hidden_dim}
    dropout: 0.3
    instance_classifiers: False
    attention_net:
      _target_: romil.models.attention.ClassAttention
      attn_scores: True # Warning if true compute inneficient attention to store the weights
      hidden_dim: ${..hidden_dim}
      attention_dim: 32
      dropout: ${..dropout}
      n_classes: ${${task}.n_classes}
      n_head: 8
      bias: True

# BTRoPEAMIL_s_pixel: ## Industrious foal
#   _target_: romil.models.RoMIL.BTRoPEAMIL
#   input_dim: 128
#   hidden_dim: 64 # Not activate first linear layer
#   absolute_position_embeddings: False
#   positional_encoder: 
#     _target_: romil.models.position_embedding_modules.RoFormerEncoder
#     _recursive_: False
#     n_attention_block: 2
#     n_embd: ${..hidden_dim}
#     n_head: 4
#     dropout: 0.3
#     rope: True
#     rope_freqs: pixel
#     resid_dropout: 0.3
#   mil_head: 
#     _target_: romil.models.ABMIL_heads.ABMIL_MB
#     n_classes: ${${task}.n_classes}
#     input_dim: ${..hidden_dim}
#     hidden_dim: ${..hidden_dim}
#     dropout: 0.3
#     instance_classifiers: False
#     attention_net:
#       _target_: romil.models.attention.ClassAttention
#       attn_scores: True # Warning if true compute inneficient attention to store the weights
#       hidden_dim: ${..hidden_dim}
#       attention_dim: 32
#       dropout: ${..dropout}
#       n_classes: ${${task}.n_classes}
#       n_head: 4
#       bias: True

# BTRoPEAMIL_s_pixel: # Invible toad
#   _target_: romil.models.RoMIL.BTRoPEAMIL
#   input_dim: 128
#   hidden_dim: 64 # Not activate first linear layer
#   absolute_position_embeddings: False
#   positional_encoder: 
#     _target_: romil.models.position_embedding_modules.RoFormerEncoder
#     _recursive_: False
#     n_attention_block: 6
#     n_embd: ${..hidden_dim}
#     n_head: 8
#     dropout: 0.3
#     rope: True
#     rope_freqs: pixel
#     resid_dropout: 0.3
#   mil_head: 
#     _target_: romil.models.ABMIL_heads.ABMIL_MB
#     n_classes: ${${task}.n_classes}
#     input_dim: ${..hidden_dim}
#     hidden_dim: ${..hidden_dim}
#     dropout: 0.3
#     instance_classifiers: False
#     attention_net:
#       _target_: romil.models.attention.ClassAttention
#       attn_scores: True # Warning if true compute inneficient attention to store the weights
#       hidden_dim: ${..hidden_dim}
#       attention_dim: 32
#       dropout: ${..dropout}
#       n_classes: ${${task}.n_classes}
#       n_head: 4
#       bias: True
