defaults:
  - defaults
  - model_dict@model_dict
  - _self_ 

task: task_4_carcinoids_CaA1_CaA2
exp_name: CaA1Ca2_unbalanced_norm  #debug
model_type: BTRoPEAMIL_s_pixel # Should be one of the models from confs/model_dict.yaml
label_frac: 100 # % of training data to use during training 

results_dir: /home/mathiane/LNENWork/data_RoFormerMIL_CaA1CaA2_norm/dataRoFormer/results/${task}/${exp_name}/label_frac=${label_frac}/${model_type}/training/${now:%Y-%m-%d}_${now:%H-%M-%S}


training_args:
  lightning_module:
    _target_: romil.models.lightning_modules.MILLitModule
    ################

    model: ${model_dict.${model_type}}
    ################

    loss :
      _target_: torch.nn.CrossEntropyLoss
    ################
  
    optimizer:
      _target_: torch.optim.Adam
      _partial_: True
      lr: 0.0005
      weight_decay: 0.0005
    ################

    lr_scheduler:
      scheduler:
        _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
        _partial_: True
        mode: "min"
        factor: 0.1
        patience: 10 ##50
        verbose: True
      monitor: val/loss

    #################################
    ### Clam Instance loss params ###
    #################################
    use_instance_loss: False
    
    k_sample: 100
    bag_loss_weight: 0.7 # Disregarded if use_instance_loss: False
    subtyping: True
    instance_loss_fn:
        _target_: torch.nn.CrossEntropyLoss


  datamodule_params:
    batch_size: 8
    batch_size_test: 1
    num_workers: 4 
    shuffle: True

  
  callbacks:
    ################

    model_checkpoint:
      _target_: pytorch_lightning.callbacks.ModelCheckpoint
      dirpath: ${results_dir}/checkpoints
      filename: "best"
      monitor: "val/loss" ##"val_BinaryAccuracy"
      mode: "min"
      save_last: True
      auto_insert_metric_name: False
    ################

    lr_monitor:
      _target_: pytorch_lightning.callbacks.LearningRateMonitor
    ################

    early_stopping:
      _target_: pytorch_lightning.callbacks.EarlyStopping
      monitor: "val/loss"
      patience: 40
      mode: "max"
      check_finite: True

  trainer: 
    _target_: pytorch_lightning.Trainer
    default_root_dir: ${results_dir}
    max_epochs: 150
    accelerator: gpu
    strategy: ddp
    precision: '16-mixed'
    # For some reason ddp + sanity_val_steps breaks lightning
    # Change to >0 when working on single gpu
    num_sanity_val_steps: 0
    #overfit_batches: 1

    logger: 
      _target_: romil.lightning_utils.MLFlowLoggerCheckpointer
      prefix: "" 
      experiment_name: ${exp_name}
      tracking_uri: "http://10.120.1.200:6084" #/home/mathiane/LNENWork/data_RoFormerMIL_CaA1CaA2_norm/dataRoFormer/data/mlflow
      run_id: 


k_folds:
  k_start: 0
  k_end: 15
